#### 5.1.8 Stackdriver Logging
* In order to preserve logs long-term, and to allow log analysis using SQL queries, we need to 
  set up Stackdriver Exports (sinks). 
* Create a BiqQuery dataset in your admin project that will be the destination for log exports:

```
PARTITION_EXPIRE=$(( 3600 * 24 * PARTITION_DAYS ))

bq --location=US mk -d --project_id ${ADMIN_PROJECT} 
   --description "Dataset for log sink in ${PROJECT}" \
   --default_partition_expiration=${PARTITION_EXPIRE} ${BQ_DATASET}
```

* Create the sync:
```

LOGNAME=compute.googleapis.com%2Ffirewall

gcloud logging sinks create ${SINK_NAME} \
   "bigquery.googleapis.com/projects/${ADMIN_PROJECT}/datasets/${BQ_DATASET}" \
    --log-filter=logName="projects/${PROJECT}/logs/${LOGNAME}" \
    --description="Log of $LOGNAME in ${PROJECT}" \
    --use-partitioned-tables \
    --project ${PROJECT}
```

* Assuming the logs are being written to a separate centralized admin project, the service account used by the
  logging system needs to be given write access to the target dataset. After creating the sink, you need to find the
  service account name being used:
```
WRITER_SA=`gcloud logging sinks describe ${SINK_NAME} --project ${PROJECT} | grep "writerIdentity" | sed -e 's/writerIdentity: serviceAccount://'`

```
* With the SA name, you then need to add it to the dataset as a writer. This Python snippet can be used:

```
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

def install_sa_as_writer(project_id, dataset_id, service_account_id):

    client = bigquery.Client(project_id)

    try:
        dataset = client.get_dataset(dataset_id)
    except NotFound:
        raise Exception("dataset %s not found" % dataset_id)

    entry = bigquery.AccessEntry(role="roles/bigquery.dataEditor", entity_type="userByEmail", entity_id=service_account_id)

    entries = list(dataset.access_entries)
    entries.append(entry)
    dataset.access_entries = entries

    dataset = client.update_dataset(dataset, ["access_entries"])
    full_dataset_id = "{}.{}".format(dataset.project, dataset.dataset_id)
    print("Updated dataset %s by adding SA %s as a writer" % (full_dataset_id, service_account_id), flush=True)
    return

```

* Logs can be persisted both in BigQuery and in storage buckets, though in IDC we currently only sink Stackdriver logs to BQ tables.


* Here are useful logs sinks, and their filters, to store logs for analysis.


| Sink Name  | Sink Filter |
| ------------- | ------------- |
| Egress-tracking  | jsonPayload.rule_details.reference:("network:solr/firewall:log-egress")  |
| Proxy-Bytes-Used | resource.type="gae_app" resource.labels.module_id="proxy" logName="projects/<project_id>/logs/stderr" textPayload:"DAILY" |
| Proxy-Exceeded | resource.type="gae_app" resource.labels.module_id="proxy" logName="projects/<project_id>/logs/stderr" textPayload:"exceeds daily threshold" |
| allWebappRequests | resource.type="gae_app" resource.labels.module_id="default" logName="projects/<project_id>/logs/appengine.googleapis.com%2Fnginx.request" |
| api_all_hits | resource.type="gae_app" resource.labels.module_id="api" logName="projects/<project_id>/logs/appengine.googleapis.com%2Fnginx.request"
| proxy_request_sink | resource.type="gae_app" resource.labels.module_id="proxy" logName="projects/<project_id>/logs/appengine.googleapis.com%2Frequest_log" |
| webapp_login_sink | logName="projects/<project_id>/logs/<tier>_webapp_login" |
| storage_audit_prod_sink | resource.type="gcs_bucket" |
| viewerHits | resource.type="http_load_balancer" resource.labels.forwarding_rule_name="<your_balancer_name>" resource.labels.url_map_name="<your_balancer_map_name>" httpRequest.requestUrl:"viewer.imaging" |
